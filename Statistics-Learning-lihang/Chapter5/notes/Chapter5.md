# $\xi5$ 决策树

[TOC]

## $\xi5.1$ 决策树模型与学习

### $\xi5.1.1$ 决策树模型

+ 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶结点。内部节点表示一个特征或属性，叶结点表示一个类。

### $\xi5.1.2$ 决策树与if-then规则

+ 可以将决策树看出一个if-then规则的集合。由决策树的根结点到叶结点的每一条路径构建一条规则；
+ 路径上内部结点的特征对应着规则的条件；而叶结点的类对应着规则的结论。
+ 决策树的路径或其对应的if-then规则具有一个重要的性质：互斥且完备。

### $\xi5.1.3$ 决策树与条件概率分布

- 决策树还表示给定特征条件下类的条件概率分布。
- 将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。
- 决策树的一条路径对应于划分中的一个单元。
- 决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。

### $\xi5.1.4$ 决策树学习

+ 决策树学习的**本质**是一个从训练数据集中归纳出一组分类规则。
+ 决策树学习的**目标**是用损失函数，通常是正则化的极大似然函数。
+ 决策树学习的**策略**是以损失函数为目标函数的最小化。
+ 决策树**构建**步骤
  + 开始构建根结点，**选择**一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集由一个在当前条件下最好的分类。
  + 子集分两种情况
    1. 能够被基本正确分类，构建叶结点
    2. 不能被基本正确分类，继续选择新特征对其进行分割。
  + 直至训练数据子集被基本正确分类或没有合适的特征为止。
+ 还需要对已生成的树自下而上进行**剪枝**，使其变得简单，从而使它具有更好的泛化能力。

## $\xi5.2$ 特征选择

### $\xi5.2.1$ 特征选择问题

+ 特征选择在于选取对训练数据具有分类能力的特征。

### $\xi5.2.2$ 信息增益

- 熵：$H(p)=H(X)=-\sum_{i=1}^{n}p_i\log p_i$

- 条件熵：$H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)$

- 当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵

- 信息增益：特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定的条件下$D$的经验条件熵$H(D|A)$之差
  $$
  g(D,A)=H(D)-H(D|A)
  $$

- 熵与条件熵的差称为互信息.

- 决策树中的信息增益等价于训练数据集中的类与特征的互信息。

- **算法5.1 信息增益算法**

  > 输入：训练数据集$D$和特征$A$
  >
  > 输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$
  >
  > 1. 数据集$D$的经验熵$H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}$
  > 2. 特征$A$对数据集$D$的经验条件熵$H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}$
  > 3. 信息增益$g(D,A)=H(D)-H(D|A)$

### $\xi5.2.3$ 信息增益比

- 考虑ID这种特征， 本身是唯一的。按照ID做划分， 得到的经验条件熵为0, 会得到最大的信息增益。所以， 按照信息增益的准则来选择特征， 可能会倾向于取值比较多的特征。而信息增益比可以对这一问题进行校正。
  $$
  g_R(D,A)=\frac{g(D,A)}{H_A(D)}\\
  H_A(D)=-\sum_{i=1}^n\frac{D_i}{D}log_2\frac{D_i}{D}
  $$
  

## $\xi5.3$ 决策树的生成

+ ID3和C4.5在生成决策树过程中，差异只在分裂准则上的差异。

### $\xi5.3.1$ ID3算法

- 输入：训练数据集$D$, 特征集$A$，阈值$\epsilon$
- 输出：决策树$T$
  1. 如果$D$属于同一类$C_k$，$T$为单节点树，类$C_k$作为该节点的类标记，返回$T$
  2. 如果$A$是空集，置$T$为单节点树，实例数最多的类作为该节点类标记，返回$T$
  3. 计算$g$, 选择**信息增益**最大的特征$A_g$
  4. 如果$A_g$的**信息增益**小于$\epsilon$，$T$为单节点树，$D$中实例数最大的类$C_k$作为类标记，返回$T$
  5. $A_g$划分若干非空子集$D_i$，
  6. $D_i$训练集，$A-A_g$为特征集，递归调用前面步骤，得到$T_i$，返回$T_i$

### $\xi5.3.2$ C4.5算法

- 输入：训练数据集$D$, 特征集$A$，阈值$\epsilon$
- 输出：决策树$T$
  1. 如果$D$属于同一类$C_k$，$T$为单节点树，类$C_k$作为该节点的类标记，返回$T$
  2. 如果$A$是空集, 置$T$为单节点树，实例数最多的作为该节点类标记，返回$T$
  3. 计算$g$, 选择**信息增益比**最大的特征$A_g$
  4. 如果$A_g$的**信息增益比**小于$\epsilon$，$T$为单节点树，$D$中实例数最大的类$C_k$作为类标记，返回$T$
  5. $A_g$划分若干非空子集$D_i$，
  6. $D_i$训练集，$A-A_g$为特征集，递归调用前面步骤，得到$T_i$，返回$T_i$

## $\xi5.4$ 决策树的剪枝

- #### 树的剪枝算法

  > 树$T$的叶结点个数为$|T|$，$t$是树$T$的叶结点，该结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$H_t(T)$为叶结点$t$上的经验熵， $\alpha\geq 0$为参数，决策树学习的损失函数可以定义为
  > $$
  > C_\alpha(T)=\sum_{i=1}^{|T|}N_tH_t(T)+\alpha|T|
  > $$
  > 其中
  > $$
  > H_t(T)=-\sum_k\color{red}\frac{N_{tk}}{N_t}\color{black}\log \frac {N_{tk}}{N_t}
  > $$
  >
  > $$
  > C(T)=\sum_{t=1}^{|T|}\color{red}N_tH_t(T)\color{black}=-\sum_{t=1}^{|T|}\sum_{k=1}^K\color{red}N_{tk}\color{black}\log\frac{N_{tk}}{N_t}
  > $$
  >
  > 这时有
  > $$
  > C_\alpha(T)=C(T)+\alpha|T|
  > $$
  > 其中$C(T)$表示模型对训练数据的误差，$|T|$表示模型复杂度，参数$\alpha \geq 0$控制两者之间的影响。

  - 输入：生成算法生成的整个树$T$，参数$\alpha$
  - 输出：修剪后的子树$T_\alpha$
    1. 计算每个结点的经验熵
    2. 递归的从树的叶结点向上回缩
       假设一组叶结点回缩到其父结点之前与之后的整体树分别是$T_B$和$T_A$，其对应的损失函数分别是$C_\alpha(T_A)$和$C_\alpha(T_B)$，如果$C_\alpha(T_A)\leq C_\alpha(T_B)$则进行剪枝，即将父结点变为新的叶结点
    3. 返回2，直至不能继续为止，得到损失函数最小的子树$T_\alpha$

## $\xi5.5$ CART算法

