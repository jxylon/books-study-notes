# $\xi5$ 决策树

[TOC]

## $\xi5.1$ 决策树模型与学习

### $\xi5.1.1$ 决策树模型

+ 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶结点。内部节点表示一个特征或属性，叶结点表示一个类。

### $\xi5.1.2$ 决策树与if-then规则

+ 可以将决策树看出一个if-then规则的集合。由决策树的根结点到叶结点的每一条路径构建一条规则；
+ 路径上内部结点的特征对应着规则的条件；而叶结点的类对应着规则的结论。
+ 决策树的路径或其对应的if-then规则具有一个重要的性质：互斥且完备。

### $\xi5.1.3$ 决策树与条件概率分布

- 决策树还表示给定特征条件下类的条件概率分布。
- 将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。
- 决策树的一条路径对应于划分中的一个单元。
- 决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。

### $\xi5.1.4$ 决策树学习

+ 决策树学习的**本质**是一个从训练数据集中归纳出一组分类规则。
+ 决策树学习的**目标**是用损失函数，通常是正则化的极大似然函数。
+ 决策树学习的**策略**是以损失函数为目标函数的最小化。
+ 决策树**构建**步骤
  + 开始构建根结点，**选择**一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集由一个在当前条件下最好的分类。
  + 子集分两种情况
    1. 能够被基本正确分类，构建叶结点
    2. 不能被基本正确分类，继续选择新特征对其进行分割。
  + 直至训练数据子集被基本正确分类或没有合适的特征为止。
+ 还需要对已生成的树自下而上进行**剪枝**，使其变得简单，从而使它具有更好的泛化能力。

## $\xi5.2$ 特征选择

### $\xi5.2.1$ 特征选择问题

+ 特征选择在于选取对训练数据具有分类能力的特征。

### $\xi5.2.2$ 信息增益

- 熵：$H(p)=H(X)=-\sum_{i=1}^{n}p_i\log p_i$

- 条件熵：$H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)$

- 当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵

- 信息增益：特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定的条件下$D$的经验条件熵$H(D|A)$之差
  $$
  g(D,A)=H(D)-H(D|A)
  $$

- 熵与条件熵的差称为互信息.

- 决策树中的信息增益等价于训练数据集中的类与特征的互信息。

- **算法5.1 信息增益算法**

  > 输入：训练数据集$D$和特征$A$
  >
  > 输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$
  >
  > 1. 数据集$D$的经验熵$H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}$
  > 2. 特征$A$对数据集$D$的经验条件熵$H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}$
  > 3. 信息增益$g(D,A)=H(D)-H(D|A)$

### $\xi5.2.3$ 信息增益比

- 考虑ID这种特征， 本身是唯一的。按照ID做划分， 得到的经验条件熵为0, 会得到最大的信息增益。所以， 按照信息增益的准则来选择特征， 可能会倾向于取值比较多的特征。而信息增益比可以对这一问题进行校正。
  $$
  g_R(D,A)=\frac{g(D,A)}{H_A(D)}\\
  H_A(D)=-\sum_{i=1}^n\frac{D_i}{D}log_2\frac{D_i}{D}
  $$
  

## $\xi5.3$ 决策树的生成

+ ID3和C4.5在生成决策树过程中，差异只在分裂准则上的差异。

### $\xi5.3.1$ ID3算法

- 输入：训练数据集$D$, 特征集$A$，阈值$\epsilon$
- 输出：决策树$T$
  1. 如果$D$属于同一类$C_k$，$T$为单节点树，类$C_k$作为该节点的类标记，返回$T$
  2. 如果$A$是空集，置$T$为单节点树，实例数最多的类作为该节点类标记，返回$T$
  3. 计算$g$, 选择**信息增益**最大的特征$A_g$
  4. 如果$A_g$的**信息增益**小于$\epsilon$，$T$为单节点树，$D$中实例数最大的类$C_k$作为类标记，返回$T$
  5. $A_g$划分若干非空子集$D_i$，
  6. $D_i$训练集，$A-A_g$为特征集，递归调用前面步骤，得到$T_i$，返回$T_i$

### $\xi5.3.2$ C4.5算法

- 输入：训练数据集$D$, 特征集$A$，阈值$\epsilon$
- 输出：决策树$T$
  1. 如果$D$属于同一类$C_k$，$T$为单节点树，类$C_k$作为该节点的类标记，返回$T$
  2. 如果$A$是空集, 置$T$为单节点树，实例数最多的作为该节点类标记，返回$T$
  3. 计算$g$, 选择**信息增益比**最大的特征$A_g$
  4. 如果$A_g$的**信息增益比**小于$\epsilon$，$T$为单节点树，$D$中实例数最大的类$C_k$作为类标记，返回$T$
  5. $A_g$划分若干非空子集$D_i$，
  6. $D_i$训练集，$A-A_g$为特征集，递归调用前面步骤，得到$T_i$，返回$T_i$

## $\xi5.4$ 决策树的剪枝

- #### 树的剪枝算法

  > 树$T$的叶结点个数为$|T|$，$t$是树$T$的叶结点，该结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$H_t(T)$为叶结点$t$上的经验熵， $\alpha\geq 0$为参数，决策树学习的损失函数可以定义为
  > $$
  > C_\alpha(T)=\sum_{i=1}^{|T|}N_tH_t(T)+\alpha|T|
  > $$
  > 其中
  > $$
  > H_t(T)=-\sum_k\color{red}\frac{N_{tk}}{N_t}\color{black}\log\frac{N_{tk}}{N_t}
  > $$
  >
  > $$
  > C(T)=\sum_{t=1}^{|T|}\color{red}N_tH_t(T)\color{black}=-\sum_{t=1}^{|T|}\sum_{k=1}^K\color{red}N_{tk}\color{black}\log\frac{N_{tk}}{N_t}
  > $$
  >
  > 这时有
  > $$
  > C_\alpha(T)=C(T)+\alpha|T|
  > $$
  > 其中$C(T)$表示模型对训练数据的误差，$|T|$表示模型复杂度，参数$\alpha \geq 0$控制两者之间的影响。

  - 输入：生成算法生成的整个树$T$，参数$\alpha$
  - 输出：修剪后的子树$T_\alpha$
    1. 计算每个结点的经验熵
    2. 递归的从树的叶结点向上回缩
       假设一组叶结点回缩到其父结点之前与之后的整体树分别是$T_B$和$T_A$，其对应的损失函数分别是$C_\alpha(T_A)$和$C_\alpha(T_B)$，如果$C_\alpha(T_A)\leq C_\alpha(T_B)$则进行剪枝，即将父结点变为新的叶结点
    3. 返回2，直至不能继续为止，得到损失函数最小的子树$T_\alpha$

## $\xi5.5$ CART算法

### $\xi5.5.1$ CART生成

+ **最小二乘回归树生成算法**

  + 输入：训练数据集$D$

  + 输出：回归树$f(x)$

  + 步骤：

    1. 遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，得到满足上面关系的$(j,s)$
       $$
       \min\limits_{j,s}\left[\min\limits_{c_1}\sum\limits_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min\limits_{c_2}\sum\limits_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]
       $$

    2. 用选定的$(j,s)$, 划分区域并决定相应的输出值
       $$
       R_1(j,s)=\{x|x^{(j)}\leq s\}, R_2(j,s)=\{x|x^{(j)}> s\} \\
       \hat{c}_m= \frac{1}{N}\sum\limits_{x_i\in R_m(j,s)} y_j, x\in R_m, m=1,2
       $$

    3. 对两个子区域调用(1)(2)步骤， 直至满足停止条件

    4. 将输入空间划分为$M$个区域$R_1, R_2,\dots,R_M$，生成决策树：
       $$
       f(x)=\sum_{m=1}^M\hat{c}_mI(x\in R_m)
       $$

+ **基尼系数定义**
  
  + $Gini(p) = \sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$
+ **CART生成算法**
  + 输入：训练集D，停止计算的条件
  + 输出：CART决策树
    1. 设结点的训练集为D，计算现有特征对该数据集的基尼指数，计算每一个特征对应的每个取值的基尼指数。
    2. 选择基尼指数最小的特征及对应的切分点作为最优特征与最优切分点。
    3. 对两个子结点递归调用步骤1和2，直至满足停止条件。
    4. 生成CART决策树
  + 停止条件
    1. 结点中的样本个数小于预定阈值。
    2. 样本集的基尼指数小于预定阈值。
    3. 没有更多特征。

### $\xi5.5.2$ CART剪枝

+ **CART剪枝算法**

  + 输入：CART算法生成的决策树$T_0$;

  + 输出：最优决策树$T_\alpha$;

    1. 设$k = 0,T= T_0$

    2. 设$\alpha = + \infty$

    3. 自下而上地对各内部结点t计算$C(T_t),|T_t|$以及
       $$
       g(t) = \frac{C(t)-C(T_t)}{|T_t|-1}\\
       \alpha = \min(\alpha,g(t))
       $$
       这里$T_t$表示以t为根结点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶结点个数。

    4. 对$g(t) = \alpha$的内部结点t进行剪枝，并对叶结点t以多数表决法决定其类，得到树T。
    5. 设$k = k+1,\alpha_k = \alpha,T_k = T$ 。（不断剪枝，得到新树$T_k$）
    6. 如果$T_k$不是由根结点及两个叶结点构成的树，则回到步骤2；否则令$T_k = T_n$。
    7. 采用交叉验证法在子树序列$T_0,T_1,...,T_n$中选取最优子树$T_\alpha$。

## $\xi5.6$ 其他

+ 预剪枝是要对划分前后泛化性能进行评估。对比决策树某节点生成前与生成后的泛化性能。
+ [图灵社区李航访谈](http://www.ituring.com.cn/article/196610)