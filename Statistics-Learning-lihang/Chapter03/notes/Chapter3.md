# $\xi3$ k近邻法

[TOC]

## $\xi3.1$ k近邻算法

+ 输入: $T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}， x_i\in \cal{X}\sube{\bf{R}^n}, y_i\in\cal{Y}=\{c_1,c_2,\dots, c_k\}$; 实例特征向量$x$

  输出: 实例所属的$y$

+ 步骤:

  1. 根据指定的**距离度量**，在$T$中查找$x$的**最近邻的$k$个点**，覆盖这$k$个点的$x$的邻域定义为$N_k(x)$

  2. 在$N_k(x)$中应用**分类决策规则**决定$x$的类别$y$
     $$
     y=\arg\max_{c_j}\sum_{x_i\in N_k(x)}I(y_i=c_j), i=1,2,\dots,N, j=1,2,\dots,K
     $$

+ $k=1$的情形, 称为最近邻算法. 书中后面的分析都是按照最近邻做例子, 这样不用判断类别, 可以略去一些细节。

+ k近邻算法没有显式的学习过程。

  + 个人理解：没有一步步对损失函数最小化的过程，而是根据训练集将特征空间直接划分，在训练集、距离度量、k值及分类决策规则确定后，对于任何一个新的实例，它所属的类是唯一确定的。

## $\xi3.2$ k近邻模型

+ k近邻模型三要素：距离度量、k值的选择、分类决策规则。

### $\xi3.2.1$ 模型

+ 在训练集、距离度量、k值及分类决策规则确定后，对于任何一个新的实例，它所属的类是唯一确定的。
+ 特征空间中，对每个训练实例点$x_i$，距离该点比其他点更近的所有点组成的一个域，叫作单元(cell)。
+ 每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分。

### $\xi3.2.2$ 距离度量

+ > **特征空间**中的两个实例点的距离是两个实例点相似程度的反映。

  书中是如上描述的，这里要注意**距离越近(数值越小), 相似度越大**。

  

+ 这里用到了$L_p$距离：

  1. $p=1$ 对应 曼哈顿距离
  2. $p=2$ 对应 欧氏距离
  3. 任意$p$ 对应 闵可夫斯基距离

+ $$L_p(x_i, x_j)=\left(\sum_{l=1}^{n}{\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^p}\right)^{\frac{1}{p}}$$

  ![fig3_2](D:/Documents/others/Lihang-first_edition/CH03/assets/fig3_2.png)

+ 考虑二维的情况, 上图给出了不同的$p$值情况下与原点距离为1的点的图形。这个图有几点理解下:

  1. 与原点的距离
  2. 与原点距离为1的点
  3. 前一点换个表达方式, 图中的点向量($x_1$, $x_2$)的$p$范数都为1
  4. 图中包含多条曲线, 关于p=1并没有对称关系
  5. 定义中$p\geqslant1$，这一组曲线中刚好是凸的

+ 范数是对向量或者矩阵的度量，是一个标量，这个里面两个点之间的$L_p$距离可以认为是两个点坐标差值的$p$范数。

### $\xi3.2.3$ k值的选择

+ k值选择会对k近邻法的结果产生重大影响。
  + 若k值越小，近似误差（训练误差）越小，但估计误差（测试误差）越大。换句话说，k值越小，模型越复杂，区域划分的越多，容易发生过拟合现象。
+ 通过**交叉验证**选取最优$k$, 算是超参数
+ 二分类问题, $k$选择奇数有助于避免平票

### $\xi3.2.4$ 分类决策规则

+ 多数表决法

+ 误分类率

  $\frac{1}{k}\sum_{x_i\in N_k(x)}{I(y_i\ne c_j)}=1-\frac{1}{k}\sum_{x_i\in N_k(x)}{I(y_i= c_j)}$

  如果分类损失函数是0-1损失, 误分类率最低即经验风险最小。

+ 多数表决规则等价于经验风险最小化。

## $\xi3.3$ k近邻法的实现：kd树

 ### $\xi3.3.1$ 构造kd树

+ 算法步骤
  1. 开始：构造根结点，根结点对应于包含T的k维空间的超矩形区域。
  2. 选择$x^{(1)}$为坐标轴，以T中所有实例的$x^{(1)}$坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并于坐标轴$x^{(1)}$垂直的超平面实现。
  3. 由根结点生成深度为1的左右子结点：左子结点对应坐标$x^{(1)}$小于切分点的子区域，右子结点对应于坐标$x^{(1)}$大于切分点的子区域。
  4. 将落在切分超平面上的实例点保存在根结点。
  5. 重复上述步骤，选择$x^{(l)}$为切分的坐标轴。直至两个子区域没有实例存在时停止，从而形成kd树的区域划分。

###  $\xi3.3.2$ 搜索kd树

+ 算法步骤
  1. 在kd树种找出包含目标点x的叶结点：
     1. 从根结点出发，递归向下访问kd树。若目标点x当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点。直到子结点为叶结点为止。
     2. 以此叶结点为当前最近点。
     3. 递归向上回退。检查零一子结点对应区域是否与目标点为球心，以目标点与当前最近点间距离为半径的超球体相交。
        1. 相交，可能在另一个子结点对应区域内存在距目标点更近的点，移动到另一个子结点，递归进行最近邻搜索。
        2. 不相交，向上回退。
     4. 当回退到根结点时，搜索结束。最后的当前最近点即为x的最近邻点。

## $\xi3.4$ 其他

+ k近邻法是一种基本分类与回归方法，可多分类（感知机适用于二分类）。
+ kNN的k和kdtree的k含义不同。
  + kNN的k是指距实例点k个最近的点。
  + kdtree的k是指k维空间存储结构。
+ kd树是二叉树，表示k维空间的一个划分，其每个结点对应于k维空间划分中的一个超矩形区域。利用kd树可以省去对大部分点的搜索，从而减少搜索的计算量。